{
  "hash": "65448d6b631f6df3c3413c348f8a2320",
  "result": {
    "markdown": "---\ntitle: 'Synthetic data: What are the properties?'\nauthor: \n- name: Kaspar Rufibach (inspired by work by Heiko GÃ¶tte and Christoph Helwig)\n  affiliation: Advanced Biostatistical Sciences, Merck KGaA, Darmstadt\ndate: last-modified\nexecute:   \n  freeze: auto  # re-render only when source changes\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    highlight-style: pygments\n---\n\n\n\n\n\n# Problem statement\n\nThere are instances where we run a study, e.g. a single-arm trial, and would like to contextualize it using external data. Ocassionally, the external data owner is not allowed or does not want to share the data. An option then is to \"learn\" the external data structure and data generating mechanism and simulate patients with these features. This is often referred to as \"synthetic data\".\n\nSynthetic data can be generated as control arm, and/or treatment arm.\n\n# Question\n\nIn theory, once one has a simulation engine, one can generate an arbitrary number of patients. The following questions then may be asked:\n\n- How many synthetic patients should we simulate?\n- Can we potentially \"replace\" and/or add real patients with simulated ones?\n\nOne can think of an arbitrary complicated data structure - to illustrate the basic feature we use here the simplest possible scenario. But the message remains the same also for more complicated scenarios.\n\n# Scenario\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- 0\nsd <- 1\n```\n:::\n\n\nWe assume that we have one measurement which is normally distributed with mean $\\mu = 0$ and standard deviation $\\sigma = 1$. \n\nNow we \"learn\" the data structure from that registry. We are interested in the mean this time, but the quantity of interest can be anything, e.g. a hazard ratio.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate registry data\nset.seed(23041977)\nn <- 100\nx <- rnorm(n = n, mean = mu, sd = sd)\n```\n:::\n\n\nSo we have measurements of this variable from $n = 100$ patients. This can the be thought of as the external registry data we want to use to contextualize our internal single arm trial, e.g.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute summary statistics of those \n# here we do that in a very simple way - \n# but AI will not get more out of it!\nmu_est <- mean(x)\nsd_est <- sd(x)\n\n# now simulate \"synthetic\" data based on the data structure that you have learned\n# from the registry\nn_syn <- 10 ^ 6\nx_syn <- rnorm(n_syn, mean = mu_est, sd = sd_est)\n```\n:::\n\n\nNow, we look at density estimates of these various quantities:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# generate an empty plot\npar(las = 1)\nplot(0, 0, type = \"n\", xlim = 4 * c(-1, 1), ylim = c(0, 0.6), xlab = \"data\", ylab = \"density\", \n     main = \"density estimates\")\nabline(h = 0)\n\n# plot the truth from which we generated the \"registry data\"\nxs <- seq(-10, 10, 0.01)\nlines(xs, dnorm(xs, mean = mu, sd = sd), col = grey(0.85), lwd = 4)\n\n# add the estimated density of the actual \"registry data\"\nlines(density(x), col = 2, lwd = 2, lty = 3)\n\n# now first dataset of \"synthetic data\": \n# use same number of patients as originally in the registry\nlines(density(x_syn[1:n]), col = 3, lwd = 2, lty = 3)\n\n# second dataset: now a million patients\nlines(density(x_syn), col = 4, lwd = 2, lty = 3)\n\nlegend(\"topleft\", c(\"truth\", \"registry data\", \"synthetic data, same size as registry\", \n                     \"synthetic data, one million patients\"),\n       lty = 1, col = c(grey(0.85), 2:4), lwd = c(3, 2, 2, 2), bty = \"n\")\n```\n\n![Estimated densities.](synthetic_files/figure-html/fig-estimates-1.png){#fig-estimates fig-align='center' width=672}\n:::\n\n\nSo, what observations can we make from this @fig-estimates?\n\n- The \"registry data\" (red line) of course comes with uncertainty, it is not \"the truth\".\n- Synthetic data, whether based on the same number of patients ($n = 100$, green) as the original registry or one million patients will \n  - mimick the registry distribution (not the underlying truth),\n  - with increasing sample size just mimick the registry distribution with a more \"normal\" shape (because that is the assumption we put in).\n  \nThe key message is that increasing the sample size beyond the number of the original dataset (our \"registry data\", $n = 100$) **does not lead to more precision beyond what is in the original dataset!** It is not possible to \"generate\" patients out of nothing.\n\n# Recommendations\n\n- The use case for \"synthetic data\" is to have data to contextualize internal data if the original data can not be used (for whatever reason). \n- If real patient data is available do not use synthetic data.\n- From a regulator's perspective \"synthetic data\" are lower in credibility than historical or external controls, because typically what we call here \"registry data\" are already historical / or external controls. But we are not using that but are simulating from some approximation to them - so they _have_ to be lower in hierarchy, because that approximation might miss one or more important aspects.\n- Even a arbitrary large number of synthetic patients cannot provide more information/value than the underlying real data (applies also/especially to rare disease setting). \n\n\n\n\n\n\n",
    "supporting": [
      "synthetic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}